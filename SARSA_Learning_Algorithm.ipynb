{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA_Learning_Algorithm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvaR3DRcpunBBcTF653p3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani94/Reinforcement_Learning/blob/main/SARSA_Learning_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9j5Oj1-Yu2K"
      },
      "source": [
        "## SARSA Algorithm for Mountain Car Example\n",
        "\n",
        "SARSA - State-Action-Reward-State-Action strategy. The agent in state S, takes an action A and get a reward R and then go to next state S'to take action A' based on the prediction of future reward points made by the agent. Using this experience, the agent can update the Q-function otherwise called as Action Value function. The new experience learned by the agent will have the direct impact on the policy which updates the Q-function and so it is called as on-policy reinforcement learning algorithm. It considers the current exploration policy for instance when a robot goes near the top of stairs by following a policy even it is dangerous, SARSA finds the optimal policy to keep the robot away from steps. It uses the discounted action from state S' and the current retrun for updatin the Q-funtion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazdCTbIfeDq"
      },
      "source": [
        "###Importing the necessary gym library which has the toolkit for developing the reinforcement learning environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWW70m4AYq1j"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Q9JcO4f_yZ"
      },
      "source": [
        "### Environment used is mountain car. The action space, observation shape, highest and lowest feature value is printed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ohyAPF4a6Bn",
        "outputId": "1c291096-e78a-4a9e-88f3-b850c0704264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "env_name = 'MountainCar-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "print(\"Action Set size :\",env.action_space)\n",
        "print(\"Observation set shape :\",env.observation_space) \n",
        "print(\"Highest state feature value :\",env.observation_space.high) \n",
        "print(\"Lowest state feature value:\",env.observation_space.low) \n",
        "print(env.observation_space.shape) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Set size : Discrete(3)\n",
            "Observation set shape : Box(2,)\n",
            "Highest state feature value : [0.6  0.07]\n",
            "Lowest state feature value: [-1.2  -0.07]\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDYf3kZmgP8q"
      },
      "source": [
        "### Assigning the Hyperparameters\n",
        "\n",
        " The number of states, episodes which an agent can learn within given steps in episodes, Learning rate, minimum learning rate, gamma value - discounted factor for agent, maximum steps - an agent can learn, epsilon - agent follows using greedy policy are defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NK4eoBna-Cp"
      },
      "source": [
        "n_states = 40  \n",
        "episodes = 10 \n",
        "\n",
        "initial_lr = 1.0 \n",
        "min_lr = 0.005\n",
        "gamma = 0.99\n",
        "max_steps = 300\n",
        "epsilon = 0.05\n",
        "\n",
        "env = env.unwrapped\n",
        "env.seed(0)         \n",
        "np.random.seed(0)   # reproduces the same random values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ybVDImRhhSW"
      },
      "source": [
        "### Performing discretization \n",
        "\n",
        "Which converts the continuous state space into discrete state space which makes it easier for the agent to learn in discrete space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xftD54TbHwG"
      },
      "source": [
        "def discretization(env, obs):\n",
        "    \n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    \n",
        "    env_den = (env_high - env_low) / n_states\n",
        "    pos_den = env_den[0]\n",
        "    vel_den = env_den[1]\n",
        "    \n",
        "    pos_high = env_high[0]\n",
        "    pos_low = env_low[0]\n",
        "    vel_high = env_high[1]\n",
        "    vel_low = env_low[1]\n",
        "    \n",
        "    pos_scaled = int((obs[0] - pos_low)/pos_den)  #converts to an integer value\n",
        "    vel_scaled = int((obs[1] - vel_low)/vel_den)  #converts to an integer value\n",
        "    \n",
        "    return pos_scaled,vel_scaled"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAg9OqiQiE2v"
      },
      "source": [
        "### SARSA Algorithm\n",
        "So far, most of the task that we used is similar to Q-learning algorithm. After this, in SARSA algoritm we will update the Q value and the corresponding Q table. \n",
        "\n",
        "Reward value is updated by taking the difference between the current position and the lowest point which is considered as the starting point from which the agent increases the reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTiRsRxabL0j"
      },
      "source": [
        "q_table = np.zeros((n_states,n_states,env.action_space.n))\n",
        "total_steps = 0\n",
        "for episode in range(episodes):\n",
        "   obs = env.reset()\n",
        "   total_reward = 0\n",
        "   # decreasing learning rate alpha over time\n",
        "   alpha = max(min_lr,initial_lr*(gamma**(episode//100)))\n",
        "   steps = 0\n",
        "\n",
        "   #action for the initial state using epsilon greedy\n",
        "   if np.random.uniform(low=0,high=1) < epsilon:\n",
        "        a = np.random.choice(env.action_space.n)\n",
        "   else:\n",
        "        pos,vel = discretization(env,obs)\n",
        "        a = np.argmax(q_table[pos][vel])\n",
        "  \n",
        "   while True:\n",
        "      # env.render()\n",
        "      pos,vel = discretization(env,obs)\n",
        "    \n",
        "      obs,reward,terminate,_ = env.step(a) \n",
        "      total_reward += abs(obs[0]+0.5) \n",
        "      pos_,vel_ = discretization(env,obs)\n",
        "\n",
        "      #action for the next state using epsilon greedy\n",
        "      if np.random.uniform(low=0,high=1) < epsilon:\n",
        "          a_ = np.random.choice(env.action_space.n)\n",
        "      else:\n",
        "          a_ = np.argmax(q_table[pos_][vel_])\n",
        "\n",
        "      #q-table update\n",
        "      q_table[pos][vel][a] = (1-alpha)*q_table[pos][vel][a] + alpha*(reward+gamma*q_table[pos_][vel_][a_])\n",
        "      steps+=1\n",
        "      if terminate:\n",
        "          break\n",
        "      a = a_\n",
        "   print(\"Episode {} completed with total reward {} in {} steps\".format(episode+1,total_reward,steps)) \n",
        "while True: #to hold the render at the last step when Car passes the flag\n",
        "   env.render()  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coG1ICuUtxKG"
      },
      "source": [
        "The following code will print the reward points for all steps in each episode. The code runs only in the local.."
      ]
    }
  ]
}